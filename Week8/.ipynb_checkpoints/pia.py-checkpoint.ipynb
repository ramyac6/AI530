{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd44e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ae975",
   "metadata": {},
   "source": [
    "\n",
    "- The states can be called 0,1,2,..., n-1.\n",
    "- The actions can also be simply called 1,2,...,m.\n",
    "-   This naming (rather than s0, s1...) makes it easy to use states and actions as indices in an array.\n",
    "-   You can represent the policy pi by a matrix with n rows and m actions. pi[i,j] = probability of taking action j in state i.\n",
    "-   To initialize pi: for every state s, look up the actions that can be taken there, and assign pi[s,a] = 1/(number of actions available at s)\n",
    "-   To initialize v: you can initialize everything to 0. Or you can initialize v[s] to one of the rewards that can be picked up at s, i.e. v(s) = R(s,a,s') for some action a and next-state s'.\n",
    "-   Your top-level function will be called pia.Â \n",
    "  - Function pia takes the following arguments:\n",
    "\n",
    "      - gamma: the discount factor, which is a float strictly between 0 and 1.\n",
    "      - The MDP description: consists in two matrices.\n",
    "        the reward array R: that's a n-by-n-by-m array, where n is the number of states in the MDP and m is the number of unique actions. R[s,s',a] = the reward for going from s to s' by taking action a. If there is no transition (s,a,s') in the MDP, then the value of R[s,s'a] is zero. Note that the same action may appear in multiple states, e.g. a car can take the action Left-Turn whether in state Parked, state Accelerating, etc.\n",
    "        The transition probability array P: this is also n-by-n-by-m, where P(s,s',a) =  probability of transitioning to s' given that agent took action a in state s.\n",
    "        These are not the most compact representations for R and P, since R contains an entry for (s,a,s') whether it's a possible transition or not, which means a lot of space is wasted. Same for P. But it'll have to do since more advanced data structures may have differences between Matlab and Python.\n",
    "   - You need to fix some order for traversing the states when doing the policy evaluation step.\n",
    "   - Decide on a termination criterion for the iterations - when does the code stop updating the policy?\n",
    "   - You will also create a tb.py file which creates sample inputs gamma, matrix R and matrix P, then calls pia on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a07f5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 87.07335459, 122.74239774]), array([[0.33016103, 0.66983897],\n",
      "       [0.37373051, 0.62626949]]))\n",
      "(array([71.63968707, 67.90845573,  0.        ,  0.        ]), array([[0.47861546, 0.52138454, 0.        , 0.        ],\n",
      "       [0.52805604, 0.47194396, 0.        , 0.        ]]))\n"
     ]
    }
   ],
   "source": [
    "def evaluation(theta, gamma, pi, state_values, P, R):\n",
    "    S = len(P) # num states\n",
    "    A = len(P[0][0]) # num actions\n",
    "\n",
    "    delta = np.inf\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        for s in range(S):\n",
    "            current_value = state_values[s]\n",
    "            new_value = 0\n",
    "            for a in range(A):\n",
    "                sum_state = 0\n",
    "                for s_ in range(S):\n",
    "                    sum_state = sum_state + P[s][s_][a] * (R[s][s_][a] + gamma * state_values[s_])\n",
    "                new_value = new_value + pi[s][a] * sum_state\n",
    "            state_values[s] = new_value\n",
    "            delta = max(delta, np.abs(state_values[s] - current_value))\n",
    "        return state_values\n",
    "    \n",
    "def improvement(gamma, pi, state_values, P, R):\n",
    "    S = len(P) # num states\n",
    "    A = len(P[0][0]) # num actions\n",
    "    stable = True\n",
    "    \n",
    "    for s in range(S):\n",
    "        old_action = np.argmax(pi[s])\n",
    "        temp = np.zeros((A))\n",
    "        for a in range(A):\n",
    "            for s_ in range(S):\n",
    "                temp[s_] = temp[s_] +  P[s][s_][a] * (R[s][s_][a] + gamma * state_values[s_])\n",
    "        pi[s] = temp / np.sum(temp)\n",
    "        if old_action != np.argmax(pi[s]): \n",
    "            stable = False\n",
    "    return stable,pi\n",
    "\n",
    "def pia(gamma, R,P):\n",
    "    S = len(P)\n",
    "    A = len(P[0][0])\n",
    "    pi = np.array([[1/A]*A for i in range(S)])\n",
    "\n",
    "    V = np.zeros(A)\n",
    "    theta = 0.001\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        V = evaluation(theta, gamma, pi, V, P, R)\n",
    "        policy_stable, pi = improvement(gamma, pi, V, P, R)\n",
    "    \n",
    "    return V, pi \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5adea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
